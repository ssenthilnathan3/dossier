---
title: Deployment Guide
description: Complete guide for deploying Dossier to production environments
---

import { Callout, Steps, Cards, Card } from 'nextra/components'

# Production Deployment Guide

Deploy Dossier to production with confidence using this comprehensive guide. We'll cover everything from basic setup to advanced production configurations with SSL, monitoring, and high availability.

## Deployment Options

<Cards>
  <Card title="ðŸ³ Docker Compose" href="#docker-compose-deployment">
    Single-server deployment using Docker Compose (Recommended for most users)
  </Card>
  <Card title="â˜¸ï¸ Kubernetes" href="#kubernetes-deployment">
    Container orchestration for high availability and auto-scaling
  </Card>
  <Card title="â˜ï¸ Cloud Platforms" href="#cloud-deployment">
    Managed deployments on AWS, GCP, Azure with infrastructure as code
  </Card>
  <Card title="ðŸ–¥ï¸ Bare Metal" href="#bare-metal-deployment">
    Direct installation on physical or virtual servers
  </Card>
</Cards>

## Docker Compose Deployment

### Prerequisites

<Callout type="warning">
**System Requirements:**
- **CPU**: 4+ cores (8+ recommended for production)
- **RAM**: 8GB minimum (16GB+ recommended)
- **Storage**: 50GB+ free space (SSD recommended)
- **Docker**: 20.10+ with Docker Compose 2.0+
- **Network**: Stable internet connection for model downloads
</Callout>

### Production Setup

<Steps>

### Clone and Configure

```bash
# Clone the repository
git clone https://github.com/your-org/dossier.git
cd dossier

# Create production environment
make prod-setup

# This creates .env.prod with production defaults
```

### Configure Environment Variables

Edit `.env.prod` with your production settings:

```env
# === PRODUCTION CONFIGURATION ===
NODE_ENV=production
DEBUG=false
LOG_LEVEL=INFO

# === FRAPPE INTEGRATION ===
FRAPPE_URL=https://your-frappe-instance.com
FRAPPE_API_KEY=your_production_api_key
FRAPPE_API_SECRET=your_production_api_secret

# === SECURITY (Generate strong secrets) ===
JWT_SECRET=your_production_jwt_secret_64_characters_minimum
WEBHOOK_SECRET=your_production_webhook_secret_32_characters
POSTGRES_PASSWORD=your_production_database_password_32_characters

# === DATABASE CONFIGURATION ===
DATABASE_URL=postgresql://dossier:${POSTGRES_PASSWORD}@postgres:5432/dossier
POSTGRES_DB=dossier
POSTGRES_USER=dossier

# === REDIS CONFIGURATION ===
REDIS_URL=redis://redis:6379
REDIS_MAXMEMORY=2gb
REDIS_MAXMEMORY_POLICY=allkeys-lru

# === VECTOR DATABASE ===
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION_SIZE=1000000

# === LLM CONFIGURATION ===
DEFAULT_MODEL=llama3.2:latest
OLLAMA_URL=http://ollama:11434
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096

# === EMBEDDING CONFIGURATION ===
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
BATCH_SIZE=32
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# === PERFORMANCE TUNING ===
WORKERS_PER_CORE=2
MAX_WORKERS=8
WORKER_TIMEOUT=300
KEEP_ALIVE=65

# === MONITORING ===
PROMETHEUS_ENABLED=true
METRICS_PORT=9090
HEALTH_CHECK_INTERVAL=30
```

### Generate Production Secrets

```bash
# Generate secure JWT secret (64+ characters)
openssl rand -hex 32

# Generate webhook secret (32+ characters)
openssl rand -hex 16

# Generate database password (32+ characters)
openssl rand -base64 32

# Store these in your .env.prod file
```

### Build Production Images

```bash
# Build optimized production images
make prod-build

# This builds multi-stage Docker images optimized for production
# with security patches and minimal attack surface
```

### Start Production Services

```bash
# Start all services in production mode
make prod-up

# Services will start with:
# - Resource limits
# - Health checks
# - Restart policies
# - Security configurations
```

### Verify Deployment

```bash
# Check service health
make health-check-prod

# Run deployment validation
python scripts/deployment-validation.py --env=production

# Test end-to-end functionality
make test-e2e-prod
```

</Steps>

## SSL/TLS Configuration

### Option 1: Nginx Reverse Proxy

Install and configure Nginx as a reverse proxy:

```nginx
# /etc/nginx/sites-available/dossier
server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com;

    # SSL Configuration
    ssl_certificate /etc/letsencrypt/live/your-domain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/your-domain.com/privkey.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;
    ssl_session_cache shared:SSL:10m;

    # Security Headers
    add_header Strict-Transport-Security "max-age=63072000" always;
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";

    # Frontend
    location / {
        proxy_pass http://localhost:3000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocket support for streaming
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }

    # API Gateway
    location /api/ {
        proxy_pass http://localhost:8080/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Increase timeouts for LLM responses
        proxy_read_timeout 300s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
    }

    # Webhook endpoint
    location /webhook {
        proxy_pass http://localhost:3001/webhook;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Enable the site:
```bash
# Enable the site
sudo ln -s /etc/nginx/sites-available/dossier /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx

# Get SSL certificate with Let's Encrypt
sudo certbot --nginx -d your-domain.com
```

### Option 2: Traefik with Docker

Add Traefik to your `docker-compose.prod.yml`:

```yaml
version: '3.8'

services:
  traefik:
    image: traefik:v2.10
    command:
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web"
      - "--certificatesresolvers.letsencrypt.acme.email=your-email@example.com"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - letsencrypt:/letsencrypt
    networks:
      - dossier-network

  frontend:
    build: ./services/frontend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=Host(`your-domain.com`)"
      - "traefik.http.routers.frontend.tls.certresolver=letsencrypt"
    networks:
      - dossier-network

  api-gateway:
    build: ./services/api-gateway
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.api.rule=Host(`your-domain.com`) && PathPrefix(`/api`)"
      - "traefik.http.routers.api.tls.certresolver=letsencrypt"
    networks:
      - dossier-network

volumes:
  letsencrypt:

networks:
  dossier-network:
    external: true
```

## Kubernetes Deployment

### Helm Chart Installation

```bash
# Add Dossier Helm repository
helm repo add dossier https://charts.dossier.dev
helm repo update

# Create namespace
kubectl create namespace dossier

# Install with custom values
helm install dossier dossier/dossier \
  --namespace dossier \
  --set global.domain=your-domain.com \
  --set frappe.url=https://your-frappe-instance.com \
  --set frappe.apiKey=your_api_key \
  --set frappe.apiSecret=your_api_secret \
  --set ingress.enabled=true \
  --set ingress.tls.enabled=true \
  --set persistence.enabled=true \
  --set monitoring.enabled=true
```

### Custom Values Configuration

Create `values.yaml`:

```yaml
# values.yaml
global:
  domain: your-domain.com
  environment: production

# Frappe Configuration
frappe:
  url: https://your-frappe-instance.com
  apiKey: your_api_key
  apiSecret: your_api_secret

# Security
security:
  jwtSecret: your_jwt_secret
  webhookSecret: your_webhook_secret

# Database Configuration
postgresql:
  enabled: true
  auth:
    username: dossier
    password: your_db_password
    database: dossier
  primary:
    persistence:
      enabled: true
      size: 50Gi
      storageClass: fast-ssd

# Redis Configuration
redis:
  enabled: true
  auth:
    enabled: false
  master:
    persistence:
      enabled: true
      size: 10Gi

# Vector Database
qdrant:
  enabled: true
  persistence:
    enabled: true
    size: 100Gi
    storageClass: fast-ssd

# Ingress Configuration
ingress:
  enabled: true
  className: nginx
  tls:
    enabled: true
    secretName: dossier-tls
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"

# Auto-scaling
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Resource Limits
resources:
  frontend:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  
  apiGateway:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  
  llmService:
    requests:
      memory: "4Gi"
      cpu: "2000m"
    limits:
      memory: "8Gi"
      cpu: "4000m"

# Monitoring
monitoring:
  enabled: true
  prometheus:
    enabled: true
  grafana:
    enabled: true
    adminPassword: your_grafana_password
```

Install with custom values:
```bash
helm install dossier dossier/dossier \
  --namespace dossier \
  --values values.yaml
```

## Cloud Platform Deployment

### AWS Deployment with Terraform

```hcl
# main.tf
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# ECS Cluster
resource "aws_ecs_cluster" "dossier" {
  name = "dossier-cluster"
  
  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

# Application Load Balancer
resource "aws_lb" "dossier" {
  name               = "dossier-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets           = aws_subnet.public[*].id

  enable_deletion_protection = true
}

# RDS PostgreSQL
resource "aws_db_instance" "postgres" {
  identifier             = "dossier-postgres"
  engine                = "postgres"
  engine_version        = "15.4"
  instance_class        = "db.t3.medium"
  allocated_storage     = 100
  storage_encrypted     = true
  
  db_name  = "dossier"
  username = "dossier"
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  skip_final_snapshot = false
  deletion_protection = true
  
  tags = {
    Name = "dossier-postgres"
  }
}

# ElastiCache Redis
resource "aws_elasticache_subnet_group" "redis" {
  name       = "dossier-redis-subnet-group"
  subnet_ids = aws_subnet.private[*].id
}

resource "aws_elasticache_replication_group" "redis" {
  replication_group_id       = "dossier-redis"
  description               = "Redis for Dossier"
  
  port                      = 6379
  parameter_group_name      = "default.redis7"
  node_type                = "cache.t3.medium"
  num_cache_clusters        = 2
  
  subnet_group_name         = aws_elasticache_subnet_group.redis.name
  security_group_ids        = [aws_security_group.redis.id]
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  
  tags = {
    Name = "dossier-redis"
  }
}

# ECS Task Definitions and Services
resource "aws_ecs_task_definition" "frontend" {
  family                   = "dossier-frontend"
  network_mode             = "awsvpc"
  requires_compatibility   = ["FARGATE"]
  cpu                      = "512"
  memory                   = "1024"
  execution_role_arn       = aws_iam_role.ecs_execution.arn
  task_role_arn           = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([
    {
      name  = "frontend"
      image = "your-account.dkr.ecr.region.amazonaws.com/dossier-frontend:latest"
      
      portMappings = [
        {
          containerPort = 3000
          protocol      = "tcp"
        }
      ]
      
      environment = [
        {
          name  = "NODE_ENV"
          value = "production"
        }
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          awslogs-group         = aws_cloudwatch_log_group.dossier.name
          awslogs-region        = var.aws_region
          awslogs-stream-prefix = "frontend"
        }
      }
    }
  ])
}

# ECS Service
resource "aws_ecs_service" "frontend" {
  name            = "dossier-frontend"
  cluster         = aws_ecs_cluster.dossier.id
  task_definition = aws_ecs_task_definition.frontend.arn
  desired_count   = 2
  launch_type     = "FARGATE"

  network_configuration {
    subnets          = aws_subnet.private[*].id
    security_groups  = [aws_security_group.ecs.id]
    assign_public_ip = false
  }

  load_balancer {
    target_group_arn = aws_lb_target_group.frontend.arn
    container_name   = "frontend"
    container_port   = 3000
  }

  depends_on = [aws_lb_listener.frontend]
}
```

### Google Cloud Platform with Cloud Run

```yaml
# cloudbuild.yaml
steps:
  # Build Docker images
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/dossier-frontend', './services/frontend']
  
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/dossier-api-gateway', './services/api-gateway']
  
  # Push images to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/dossier-frontend']
  
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/dossier-api-gateway']
  
  # Deploy to Cloud Run
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'dossier-frontend'
      - '--image'
      - 'gcr.io/$PROJECT_ID/dossier-frontend'
      - '--region'
      - 'us-central1'
      - '--platform'
      - 'managed'
      - '--allow-unauthenticated'
```

## Monitoring and Observability

### Prometheus and Grafana Setup

```yaml
# monitoring/docker-compose.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=your_admin_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ./dashboards:/etc/grafana/provisioning/dashboards
      - ./datasources:/etc/grafana/provisioning/datasources

volumes:
  prometheus_data:
  grafana_data:
```

Prometheus configuration:
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'dossier-services'
    static_configs:
      - targets:
        - 'api-gateway:8080'
        - 'ingestion-service:8001'
        - 'embedding-service:8002'
        - 'query-service:8003'
        - 'llm-service:8004'
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
```

### Log Aggregation with ELK Stack

```yaml
# logging/docker-compose.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

## Backup and Recovery

### Database Backup Script

```bash
#!/bin/bash
# scripts/backup-postgres.sh

BACKUP_DIR="/backups/postgres"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
CONTAINER_NAME="postgres"
DATABASE_NAME="dossier"

# Create backup directory
mkdir -p ${BACKUP_DIR}

# Create backup
docker exec ${CONTAINER_NAME} pg_dump -U dossier -d ${DATABASE_NAME} | gzip > ${BACKUP_DIR}/dossier_backup_${TIMESTAMP}.sql.gz

# Upload to S3 (optional)
aws s3 cp ${BACKUP_DIR}/dossier_backup_${TIMESTAMP}.sql.gz s3://your-backup-bucket/postgres/

# Clean up old backups (keep last 7 days)
find ${BACKUP_DIR} -name "*.sql.gz" -mtime +7 -delete

echo "Backup completed: dossier_backup_${TIMESTAMP}.sql.gz"
```

### Vector Database Backup

```bash
#!/bin/bash
# scripts/backup-qdrant.sh

BACKUP_DIR="/backups/qdrant"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
QDRANT_URL="http://localhost:6333"

# Create backup directory
mkdir -p ${BACKUP_DIR}

# Create snapshot
curl -X POST "${QDRANT_URL}/collections/documents/snapshots"

# Wait for snapshot creation
sleep 10

# List snapshots and get the latest
SNAPSHOT_NAME=$(curl -s "${QDRANT_URL}/collections/documents/snapshots" | jq -r '.result[-1].name')

# Download snapshot
curl -X GET "${QDRANT_URL}/collections/documents/snapshots/${SNAPSHOT_NAME}" --output ${BACKUP_DIR}/qdrant_snapshot_${TIMESTAMP}.tar

# Upload to S3 (optional)
aws s3 cp ${BACKUP_DIR}/qdrant_snapshot_${TIMESTAMP}.tar s3://your-backup-bucket/qdrant/

echo "Qdrant backup completed: qdrant_snapshot_${TIMESTAMP}.tar"
```

### Automated Backup with Cron

```bash
# Add to crontab
# Database backup every 6 hours
0 */6 * * * /path/to/dossier/scripts/backup-postgres.sh

# Vector database backup daily at 2 AM
0 2 * * * /path/to/dossier/scripts/backup-qdrant.sh

# System health check every 5 minutes
*/5 * * * * /path/to/dossier/scripts/health-check.sh
```

## Performance Optimization

### Production Tuning

```env
# .env.prod - Performance optimizations

# PostgreSQL Tuning
POSTGRES_SHARED_BUFFERS=2GB
POSTGRES_EFFECTIVE_CACHE_SIZE=6GB
POSTGRES_MAINTENANCE_WORK_MEM=512MB
POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
POSTGRES_WAL_BUFFERS=16MB
POSTGRES_DEFAULT_STATISTICS_TARGET=100

# Redis Tuning
REDIS_MAXMEMORY=4gb
REDIS_MAXMEMORY_POLICY=allkeys-lru
REDIS_SAVE=""  # Disable RDB snapshots for performance
REDIS_APPENDONLY=yes
REDIS_APPENDFSYNC=everysec

# Application Tuning
WORKERS_PER_CORE=2
MAX_WORKERS=16
WORKER_TIMEOUT=300
KEEP_ALIVE=65
BATCH_SIZE=64
CONNECTION_POOL_SIZE=20

# Embedding Service Optimization
EMBEDDING_BATCH_SIZE=64
EMBEDDING_CACHE_SIZE=10000
ONNX_OPTIMIZATION=true

# LLM Service Optimization
OLLAMA_NUM_PARALLEL=2
OLLAMA_NUM_THREAD=8
OLLAMA_NUM_GPU=1  # If GPU available
```

### Resource Limits in Docker

```yaml
# docker-compose.prod.yml - Resource limits
version: '3.8'

services:
  frontend:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  api-gateway:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  llm-service:
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  postgres:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
```

## Security Checklist

### Production Security Configuration

- [ ] **Strong Secrets**: Generate cryptographically secure secrets
- [ ] **SSL/TLS**: Enable HTTPS with valid certificates
- [ ] **Firewall**: Configure network access controls
- [ ] **Container Security**: Use minimal base images and security scanning
- [ ] **Database Security**: Enable encryption at rest and in transit
- [ ] **Access Controls**: Implement least privilege principles
- [ ] **Rate Limiting**: Configure appropriate rate limits
- [ ] **Security Headers**: Set security headers in reverse proxy
- [ ] **Regular Updates**: Keep all components updated
- [ ] **Vulnerability Scanning**: Regular security assessments

### Environment-Specific Security

```bash
# Generate production secrets
export JWT_SECRET=$(openssl rand -hex 32)
export WEBHOOK_SECRET=$(openssl rand -hex 16)
export POSTGRES_PASSWORD=$(openssl rand -base64 32)

# Set restrictive file permissions
chmod 600 .env.prod
chmod +x scripts/*.sh

# Configure Docker security
echo '{"log-driver": "json-file", "log-opts": {"max-size": "10m", "max-file": "3"}}' > /etc/docker/daemon.json
```

## Troubleshooting Production Issues

### Common Production Problems

#### High Memory Usage
```bash
# Check memory usage by service
docker stats --no-stream

# Check Qdrant memory usage
curl http://localhost:6333/telemetry

# Optimize Qdrant configuration
curl -X PUT "http://localhost:6333/collections/documents" \
  -H 'Content-Type: application/json' \
  -d '{"optimizer_config": {"memmap_threshold": 20000}}'
```

#### Slow Query Performance
```bash
# Check PostgreSQL slow queries
docker exec postgres psql -U dossier -d dossier -c "
SELECT query, mean_exec_time, calls 
FROM pg_stat_statements 
ORDER BY mean_exec_time DESC 
LIMIT 10;"

# Check Qdrant performance metrics
curl http://localhost:6333/metrics
```

#### Service Connection Issues
```bash
# Check service connectivity
docker network inspect dossier_default

# Test inter-service communication
docker exec api-gateway curl -f http://query-service:8003/health
docker exec query-service curl -f http://qdrant:6333/health
```

### Monitoring and Alerting

Set up alerts for critical metrics:

```yaml
# alerts.yml - Prometheus alert rules
groups:
  - name: dossier
    rules:
      - alert: HighErrorRate
        expr: rate(dossier_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High error rate detected

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(dossier_response_duration_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: High response time detected

      - alert: ServiceDown
        expr: up{job="dossier-services"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Service is down
```

---

<Callout type="success">
**Production Ready!** ðŸš€ Your Dossier Live RAG system is now deployed and ready for production use with enterprise-grade security, monitoring, and performance optimizations.
</Callout>

For ongoing maintenance and updates, refer to our [Operations Guide](/guides/operations) and [Troubleshooting Guide](/troubleshooting).